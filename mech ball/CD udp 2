import cv2
import cv2.aruco as aruco
import numpy as np
import time
from collections import deque, Counter
import socket
import struct
import math

# ----------------------------
# CONFIG
# ----------------------------
ROBOT1_ID = 1                 # Target robot marker ID
ROBOT2_ID = 2                 # Chaser robot marker ID

MAX_HISTORY = 30              # points used for velocity estimation
ALPHA_POS = 0.35              # EMA smoothing for position
MIN_STEP_PX = 2               # ignore jitter smaller than this

# Prediction / drawing
PATH_LEN_PX = 800             # constant predicted path length on screen (tune)
MIN_SPEED_PX_S = 15.0         # below this, heading is unreliable (tune)

# Bitflip / robustness controls
USE_CLAHE = True              # glare robustness
USE_BILATERAL = True          # denoise without destroying edges (moderate cost)
ID_VOTE_WINDOW = 7            # majority vote window for each robot ID
ID_MIN_VOTES = 4              # require >= this many votes to accept ID
GATE_DIST_PX = 25             # if ID mismatch but position near last, treat as bitflip
LOST_TIMEOUT_S = 0.35         # keep last state for this long if marker briefly lost

# Camera control (if supported by your camera driver)
TRY_DISABLE_AUTO_EXPOSURE = True
MANUAL_EXPOSURE = -6          # often negative for webcams; depends on camera/driver
MANUAL_GAIN = 0               # depends on camera/driver

# ----------------------------
# UDP OUTPUT (Robot2 angle-to-IP)
# ----------------------------
UDP_IP = "138.38.229.153"      # <-- set to receiver (Simulink/PC) IP
UDP_PORT = 50002              # <-- set to receiver port
sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)

# If your robot turns the wrong way, set True (sign flip)
FLIP_ANGLE_SIGN = False

# If the marker's "top edge" is not the robot's forward direction, set an offset (radians)
HEADING_OFFSET_RAD = 0.0

# ----------------------------
# IP STABILITY / SEND-ONCE CONTROL
# ----------------------------
IP_STABLE_PX = 6.0            # max IP movement per frame to be considered "stable"
IP_STABLE_FRAMES = 10         # consecutive stable frames required before sending
IP_RESET_JUMP_PX = 40.0       # if IP jumps more than this, re-arm sending
IP_LOST_RESET_S = 0.30        # if no IP for this long, re-arm sending

# ----------------------------
# ArUco setup
# ----------------------------
aruco_dict = aruco.getPredefinedDictionary(aruco.DICT_4X4_50)
parameters = aruco.DetectorParameters()

# Detector tuning for glare/noisy thresholding
parameters.adaptiveThreshWinSizeMin = 3
parameters.adaptiveThreshWinSizeMax = 35
parameters.adaptiveThreshWinSizeStep = 4

parameters.cornerRefinementMethod = aruco.CORNER_REFINE_SUBPIX
parameters.cornerRefinementWinSize = 5
parameters.cornerRefinementMaxIterations = 30
parameters.cornerRefinementMinAccuracy = 0.1

parameters.minMarkerPerimeterRate = 0.03
parameters.maxMarkerPerimeterRate = 4.0
parameters.perspectiveRemoveIgnoredMarginPerCell = 0.2
parameters.perspectiveRemovePixelPerCell = 8

cap = cv2.VideoCapture(1)
cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1920)
cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 1080)

# Try to reduce saturation from glare by reducing auto exposure / gain
if TRY_DISABLE_AUTO_EXPOSURE:
    try:
        cap.set(cv2.CAP_PROP_AUTO_EXPOSURE, 0.25)  # common: 0.25 = manual for some backends
        cap.set(cv2.CAP_PROP_EXPOSURE, float(MANUAL_EXPOSURE))
        cap.set(cv2.CAP_PROP_GAIN, float(MANUAL_GAIN))
    except Exception:
        pass

cv2.namedWindow("frame", cv2.WINDOW_AUTOSIZE)

# ----------------------------
# Helpers
# ----------------------------
clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8)) if USE_CLAHE else None

def preprocess_gray(frame_bgr: np.ndarray) -> np.ndarray:
    gray = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2GRAY)
    if USE_CLAHE and clahe is not None:
        gray = clahe.apply(gray)
    if USE_BILATERAL:
        gray = cv2.bilateralFilter(gray, 5, 50, 50)
    return gray

def centroid_from_corners(corners_4x2: np.ndarray) -> np.ndarray:
    return np.mean(corners_4x2, axis=0)

def estimate_velocity(history_deque):
    """Least-squares fit x(t), y(t) -> velocity (vx,vy) in px/s."""
    if len(history_deque) < 6:
        return None
    t = np.array([h[0] for h in history_deque], dtype=np.float64)
    p = np.array([h[1] for h in history_deque], dtype=np.float64)  # (N,2)
    t0 = t[0]
    tt = t - t0
    A = np.vstack([tt, np.ones_like(tt)]).T  # (N,2)
    vx, _bx = np.linalg.lstsq(A, p[:, 0], rcond=None)[0]
    vy, _by = np.linalg.lstsq(A, p[:, 1], rcond=None)[0]
    return np.array([float(vx), float(vy)], dtype=np.float64)

def draw_x(img, pt, size=12, color=(255, 0, 255), thickness=3):
    x, y = pt
    cv2.line(img, (x - size, y - size), (x + size, y + size), color, thickness, cv2.LINE_AA)
    cv2.line(img, (x - size, y + size), (x + size, y - size), color, thickness, cv2.LINE_AA)

def dist2(a, b):
    dx = float(a[0] - b[0])
    dy = float(a[1] - b[1])
    return dx*dx + dy*dy

def majority_vote(id_deque):
    """Return (winner_id, votes) or (None, 0) if empty."""
    if not id_deque:
        return None, 0
    c = Counter(id_deque)
    winner, votes = c.most_common(1)[0]
    return int(winner), int(votes)

def wrap_to_pi(a: float) -> float:
    """Wrap angle to [-pi, pi]."""
    return (a + math.pi) % (2.0 * math.pi) - math.pi

def angle_from_vec(vx: float, vy: float) -> float:
    """Angle of vector in image coords (x right, y down)."""
    return math.atan2(vy, vx)

# ----------------------------
# State buffers
# ----------------------------
hist1 = deque(maxlen=MAX_HISTORY)  # (t, [x,y])
hist2 = deque(maxlen=MAX_HISTORY)

trail1 = deque(maxlen=400)
pos1_filt = None
pos2_filt = None
last_trail1_pt = None

# ID voting buffers (to mitigate bitflips)
id_votes_r1 = deque(maxlen=ID_VOTE_WINDOW)
id_votes_r2 = deque(maxlen=ID_VOTE_WINDOW)

# Last-seen time for dropout handling
last_seen_r1_t = None
last_seen_r2_t = None

# IP stability / send-once state
ip_last = None
ip_stable_count = 0
ip_last_seen_t = None
sent_once = False

while True:
    loop_start = time.perf_counter()

    ret, frame = cap.read()
    if not ret or frame is None:
        print("Failed to grab frame.")
        break

    now = time.perf_counter()
    gray = preprocess_gray(frame)

    corners, ids, rejected = aruco.detectMarkers(gray, aruco_dict, parameters=parameters)

    out = frame.copy()

    # Default: not seen this frame
    seen1 = False
    seen2 = False
    p1 = None
    p2 = None
    r2_corners = None  # store chosen Robot2 marker corners for heading

    # --- detect & draw ---
    if ids is not None and len(ids) > 0:
        ids_flat = ids.flatten().astype(int)
        aruco.drawDetectedMarkers(out, corners, ids)

        # Build list of detections with centroid for gating / picking the correct one
        detections = []
        for i, mid in enumerate(ids_flat):
            c = corners[i][0]  # (4,2)
            pt = centroid_from_corners(c)
            detections.append((int(mid), pt, i))

        # Feed vote buffers with any occurrences of expected IDs
        for mid, pt, i in detections:
            if mid == ROBOT1_ID:
                id_votes_r1.append(mid)
            if mid == ROBOT2_ID:
                id_votes_r2.append(mid)

        # Apply ID voting (stabilise presence)
        r1_id, r1_votes = majority_vote(id_votes_r1)
        r2_id, r2_votes = majority_vote(id_votes_r2)

        r1_allowed = (r1_id == ROBOT1_ID and r1_votes >= ID_MIN_VOTES)
        r2_allowed = (r2_id == ROBOT2_ID and r2_votes >= ID_MIN_VOTES)

        # Choose the best detection for each robot:
        # Prefer exact ID; otherwise allow gated "near last position" candidate (bitflip mitigation)
        def pick_robot_detection(expected_id, last_pos_filt, allowed):
            if not detections:
                return None

            # 1) If allowed, choose detections with expected ID; if multiple, choose nearest to last_pos (or first)
            candidates = [(mid, pt, i) for (mid, pt, i) in detections if mid == expected_id]
            if allowed and candidates:
                if last_pos_filt is None:
                    return candidates[0]
                candidates.sort(key=lambda x: dist2(x[1], last_pos_filt))
                return candidates[0]

            # 2) If not allowed (or no expected ID found), allow gated recovery:
            if last_pos_filt is not None:
                close = [(mid, pt, i) for (mid, pt, i) in detections
                         if dist2(pt, last_pos_filt) <= GATE_DIST_PX*GATE_DIST_PX]
                if close:
                    close.sort(key=lambda x: dist2(x[1], last_pos_filt))
                    return close[0]

            return None

        pick1 = pick_robot_detection(ROBOT1_ID, pos1_filt, r1_allowed)
        pick2 = pick_robot_detection(ROBOT2_ID, pos2_filt, r2_allowed)

        # Update Robot 1 state if selected
        if pick1 is not None:
            mid, pt, idx = pick1
            seen1 = True
            last_seen_r1_t = now

            if pos1_filt is None:
                pos1_filt = pt.copy()
            else:
                pos1_filt = (1.0 - ALPHA_POS) * pos1_filt + ALPHA_POS * pt

            p1 = pos1_filt
            hist1.append((now, p1.copy()))

            p1_int = (int(p1[0]), int(p1[1]))
            if last_trail1_pt is None:
                trail1.append(p1_int)
                last_trail1_pt = p1_int
            else:
                dx = p1_int[0] - last_trail1_pt[0]
                dy = p1_int[1] - last_trail1_pt[1]
                if dx*dx + dy*dy >= MIN_STEP_PX*MIN_STEP_PX:
                    trail1.append(p1_int)
                    last_trail1_pt = p1_int

            cv2.circle(out, p1_int, 6, (0, 255, 0), -1)
            cv2.putText(out, f"R1 ID:{ROBOT1_ID}", (p1_int[0] + 10, p1_int[1] - 10),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2, cv2.LINE_AA)

        # Update Robot 2 state if selected
        if pick2 is not None:
            mid, pt, idx = pick2
            seen2 = True
            last_seen_r2_t = now

            # Store corners for heading calculation
            r2_corners = corners[idx][0]  # (4,2)

            if pos2_filt is None:
                pos2_filt = pt.copy()
            else:
                pos2_filt = (1.0 - ALPHA_POS) * pos2_filt + ALPHA_POS * pt

            p2 = pos2_filt
            hist2.append((now, p2.copy()))

            p2_int = (int(p2[0]), int(p2[1]))
            cv2.circle(out, p2_int, 6, (255, 255, 0), -1)
            cv2.putText(out, f"R2 ID:{ROBOT2_ID}", (p2_int[0] + 10, p2_int[1] - 10),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 0), 2, cv2.LINE_AA)

    # --- handle brief dropouts (use last known state) ---
    if not seen1 and last_seen_r1_t is not None and (now - last_seen_r1_t) <= LOST_TIMEOUT_S:
        p1 = pos1_filt
        seen1 = p1 is not None
    if not seen2 and last_seen_r2_t is not None and (now - last_seen_r2_t) <= LOST_TIMEOUT_S:
        p2 = pos2_filt
        seen2 = p2 is not None
        # Note: corners are not available during dropout; won't send unless r2_corners exists.

    # If IP has been missing for a while, re-arm sending
    if ip_last_seen_t is not None and (now - ip_last_seen_t) > IP_LOST_RESET_S:
        ip_stable_count = 0
        ip_last = None
        sent_once = False
        ip_last_seen_t = None

    # Draw Robot 1 past trail
    if len(trail1) >= 2:
        pts = np.array(trail1, dtype=np.int32).reshape((-1, 1, 2))
        cv2.polylines(out, [pts], False, (0, 255, 255), 2)

    # --- Predicted path + "IP" (foot of perpendicular) ---
    v1 = estimate_velocity(hist1)
    if seen1 and p1 is not None and v1 is not None:
        speed = float(np.linalg.norm(v1))

        cv2.putText(out, f"R1 v(px/s)=({v1[0]:.1f},{v1[1]:.1f}) | |v|={speed:.1f}",
                    (20, 80), cv2.FONT_HERSHEY_SIMPLEX, 0.9,
                    (255, 255, 255), 2, cv2.LINE_AA)

        if speed >= MIN_SPEED_PX_S:
            u = (v1 / speed).astype(np.float64)              # unit heading direction
            A = p1.astype(np.float64)                        # start at robot1
            B = (p1 + u * float(PATH_LEN_PX)).astype(np.float64)

            A_int = (int(A[0]), int(A[1]))
            B_int = (int(B[0]), int(B[1]))
            cv2.line(out, A_int, B_int, (255, 0, 255), 3, cv2.LINE_AA)

            # Perpendicular "IP" from Robot 2 to R1 predicted line segment
            if seen2 and p2 is not None:
                P = p2.astype(np.float64)

                t = float(np.dot(P - A, u))
                t_clamped = float(np.clip(t, 0.0, float(PATH_LEN_PX)))
                F = A + u * t_clamped

                F_int = (int(F[0]), int(F[1]))
                P_int = (int(P[0]), int(P[1]))

                cv2.line(out, P_int, F_int, (255, 0, 255), 2, cv2.LINE_AA)
                draw_x(out, F_int, size=14, color=(255, 0, 255), thickness=3)
                cv2.putText(out, "IP", (F_int[0] + 10, F_int[1] - 10),
                            cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 0, 255), 2, cv2.LINE_AA)

                # ----------------------------
                # IP stability tracking
                # ----------------------------
                ip_last_seen_t = now
                if ip_last is None:
                    ip_last = F.copy()
                    ip_stable_count = 0
                else:
                    step = float(np.linalg.norm(F - ip_last))

                    # If IP teleports, re-arm immediately
                    if step > IP_RESET_JUMP_PX:
                        ip_stable_count = 0
                        sent_once = False
                    else:
                        # Count stability if it barely moves
                        if step <= IP_STABLE_PX:
                            ip_stable_count += 1
                        else:
                            ip_stable_count = 0

                    ip_last = F.copy()

                ip_is_stable = (ip_stable_count >= IP_STABLE_FRAMES)

                cv2.putText(out, f"IP stable: {ip_is_stable} ({ip_stable_count}/{IP_STABLE_FRAMES})",
                            (20, 215), cv2.FONT_HERSHEY_SIMPLEX, 0.7,
                            (255, 255, 255), 2, cv2.LINE_AA)

                # ----------------------------
                # SEND UDP ONCE (only after IP is stable)
                # ----------------------------
                if (not sent_once) and ip_is_stable and (r2_corners is not None):
                    # Corner order (OpenCV ArUco): 0=TL, 1=TR, 2=BR, 3=BL
                    c0 = r2_corners[0]
                    c1 = r2_corners[1]

                    # Robot2 heading direction vector in image coordinates
                    hx = float(c1[0] - c0[0])
                    hy = float(c1[1] - c0[1])

                    heading = angle_from_vec(hx, hy)
                    heading = wrap_to_pi(heading + HEADING_OFFSET_RAD)

                    # Desired direction from Robot2 (P) to Intercept Point (F)
                    dx = float(F[0] - P[0])
                    dy = float(F[1] - P[1])
                    desired = angle_from_vec(dx, dy)

                    ang_err = wrap_to_pi(desired - heading)
                    if FLIP_ANGLE_SIGN:
                        ang_err = -ang_err

                    # Send as float32 little-endian
                    sock.sendto(struct.pack('<f', float(ang_err)), (UDP_IP, UDP_PORT))
                    sent_once = True

                    cv2.putText(out, f"SENT ONCE ang_err(rad)={ang_err:+.3f}",
                                (20, 245), cv2.FONT_HERSHEY_SIMPLEX, 0.7,
                                (255, 255, 255), 2, cv2.LINE_AA)

        else:
            cv2.putText(out, "Predicted path: speed too low / heading unstable",
                        (20, 115), cv2.FONT_HERSHEY_SIMPLEX, 0.9,
                        (0, 0, 255), 2, cv2.LINE_AA)
    else:
        cv2.putText(out, "R1 prediction: waiting for enough samples / stable detection",
                    (20, 80), cv2.FONT_HERSHEY_SIMPLEX, 0.9,
                    (255, 255, 255), 2, cv2.LINE_AA)

    # HUD
    loop_end = time.perf_counter()
    fps = 1.0 / (loop_end - loop_start + 1e-9)
    cv2.putText(out, f"FPS: {fps:.1f} | 'c' clear | 'q' quit",
                (20, 40), cv2.FONT_HERSHEY_SIMPLEX, 0.9,
                (255, 255, 255), 2, cv2.LINE_AA)

    # Optional: show rejected count as a glare/noise indicator
    rej_n = 0 if rejected is None else len(rejected)
    cv2.putText(out, f"Rejected: {rej_n} | Votes R1:{len(id_votes_r1)} R2:{len(id_votes_r2)}",
                (20, 150), cv2.FONT_HERSHEY_SIMPLEX, 0.75,
                (255, 255, 255), 2, cv2.LINE_AA)

    cv2.imshow("frame", out)

    key = cv2.waitKey(1) & 0xFF
    if key == ord('q'):
        break
    if key == ord('c'):
        hist1.clear()
        hist2.clear()
        trail1.clear()
        pos1_filt = None
        pos2_filt = None
        last_trail1_pt = None
        id_votes_r1.clear()
        id_votes_r2.clear()
        last_seen_r1_t = None
        last_seen_r2_t = None

        # reset IP stability + send-once
        ip_last = None
        ip_stable_count = 0
        ip_last_seen_t = None
        sent_once = False

cap.release()
cv2.destroyAllWindows()
exit(0)
